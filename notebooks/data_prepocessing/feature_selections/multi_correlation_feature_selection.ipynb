{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49cc4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr, f_oneway\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d04d42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_ratio(categories, values):\n",
    "    \"\"\"Compute correlation ratio for categorical-continuous association\"\"\"\n",
    "    categories = np.array(categories)\n",
    "    values = np.array(values)\n",
    "    f_cat, _ = pd.factorize(categories)\n",
    "    cat_num = np.max(f_cat) + 1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    \n",
    "    for i in range(cat_num):\n",
    "        cat_measures = values[np.argwhere(f_cat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.mean(cat_measures)\n",
    "    \n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array, n_array)) / np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array, np.power(np.subtract(y_avg_array, y_total_avg), 2)))\n",
    "    denominator = np.sum(np.power(np.subtract(values, y_total_avg), 2))\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    return np.sqrt(numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ad7028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_ratio(categories, values):\n",
    "    \"\"\"Compute correlation ratio for categorical-continuous association\"\"\"\n",
    "    categories = np.array(categories)\n",
    "    values = np.array(values)\n",
    "    f_cat, _ = pd.factorize(categories)\n",
    "    cat_num = np.max(f_cat) + 1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    \n",
    "    for i in range(cat_num):\n",
    "        cat_measures = values[np.argwhere(f_cat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.mean(cat_measures)\n",
    "    \n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array, n_array)) / np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array, np.power(np.subtract(y_avg_array, y_total_avg), 2)))\n",
    "    denominator = np.sum(np.power(np.subtract(values, y_total_avg), 2))\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    return np.sqrt(numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74e03dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_correlation_fs_regression(\n",
    "    X,  # DataFrame of features (mixed types)\n",
    "    y,  # Target Series\n",
    "    metrics=['pearson', 'mi'],  # Metrics to use\n",
    "    redundancy_threshold=0.8,   # Max allowed correlation between features\n",
    "    top_k=None,                 # Max features to select\n",
    "    random_state=None,          # Seed for reproducibility\n",
    "    categorical_features=None   # List of categorical feature names\n",
    "):\n",
    "    \"\"\"\n",
    "    Feature selection for regression handling both numerical and categorical features\n",
    "    Returns selected feature names and their scores\n",
    "    \"\"\"\n",
    "    # Identify categorical features if not provided\n",
    "    if categorical_features is None:\n",
    "        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "    \n",
    "    # Preprocessing pipeline for numerical features\n",
    "    num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Preprocessing pipeline for categorical features\n",
    "    cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    # Full preprocessing\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', num_pipeline, numerical_features),\n",
    "        ('cat', cat_pipeline, categorical_features)\n",
    "    ])\n",
    "    \n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    X_processed = pd.DataFrame(X_processed, \n",
    "                               columns=(numerical_features + \n",
    "                                        list(preprocessor.named_transformers_['cat']\n",
    "                                             .named_steps['encoder']\n",
    "                                             .get_feature_names_out(categorical_features))))\n",
    "    \n",
    "    # Initialize scores DataFrame\n",
    "    scores = pd.DataFrame(index=X.columns)\n",
    "    abs_scores = pd.DataFrame(index=X.columns)\n",
    "    \n",
    "    # Compute relevance metrics for each feature\n",
    "    for col in X.columns:\n",
    "        if col in numerical_features:\n",
    "            # Numerical feature metrics\n",
    "            if 'pearson' in metrics:\n",
    "                r, _ = pearsonr(X[col].fillna(X[col].median()), y)\n",
    "                scores.loc[col, 'pearson'] = r\n",
    "                abs_scores.loc[col, 'pearson'] = abs(r)\n",
    "                \n",
    "            if 'spearman' in metrics:\n",
    "                r, _ = spearmanr(X[col].fillna(X[col].median()), y)\n",
    "                scores.loc[col, 'spearman'] = r\n",
    "                abs_scores.loc[col, 'spearman'] = abs(r)\n",
    "                \n",
    "        else:  # Categorical feature\n",
    "            # Correlation ratio (eta)\n",
    "            if 'pearson' in metrics:\n",
    "                eta = correlation_ratio(X[col], y)\n",
    "                scores.loc[col, 'pearson'] = eta\n",
    "                abs_scores.loc[col, 'pearson'] = eta\n",
    "                \n",
    "            # ANOVA F-value (converted to correlation-like measure)\n",
    "            if 'spearman' in metrics:\n",
    "                groups = [y[X[col] == cat] for cat in X[col].unique()]\n",
    "                f, _ = f_oneway(*groups)\n",
    "                scores.loc[col, 'spearman'] = f\n",
    "                abs_scores.loc[col, 'spearman'] = f\n",
    "    \n",
    "    # Mutual Information (handles both types after encoding)\n",
    "    if 'mi' in metrics:\n",
    "        # Mark all one-hot encoded columns as discrete\n",
    "        discrete_features = [\n",
    "            i for i, col in enumerate(X_processed.columns)\n",
    "            if any([col.startswith(cat + '_') or col == cat for cat in categorical_features])\n",
    "        ]\n",
    "        mi_scores = mutual_info_regression(\n",
    "            X_processed, y, \n",
    "            random_state=random_state,\n",
    "            discrete_features=discrete_features\n",
    "        )\n",
    "        mi_df = pd.Series(mi_scores, index=X_processed.columns)\n",
    "        \n",
    "        # Aggregate MI for original categorical features (sum of one-hot components)\n",
    "        for col in X.columns:\n",
    "            if col in categorical_features:\n",
    "                # Sum MI for all one-hot components of this categorical feature\n",
    "                components = [c for c in mi_df.index if c.startswith(col + '_') or c == col]\n",
    "                scores.loc[col, 'mi'] = mi_df[components].sum() if components else 0\n",
    "            else:\n",
    "                scores.loc[col, 'mi'] = mi_df[col]\n",
    "                \n",
    "        abs_scores['mi'] = scores['mi']\n",
    "    \n",
    "    # Normalize scores (0 to 1) and combine\n",
    "    normalized = abs_scores.apply(lambda x: (x - x.min()) / (x.max() - x.min() + 1e-10))\n",
    "    normalized['combined'] = normalized.mean(axis=1)\n",
    "    \n",
    "    # Sort features by combined score\n",
    "    sorted_features = normalized['combined'].sort_values(ascending=False).index\n",
    "    \n",
    "    # Create encoded version for redundancy check\n",
    "    le = LabelEncoder()\n",
    "    X_encoded = X.copy()\n",
    "    for col in categorical_features:\n",
    "        X_encoded[col] = le.fit_transform(X[col].astype(str))\n",
    "    \n",
    "    # Select features with redundancy control\n",
    "    selected = []\n",
    "    for feature in sorted_features:\n",
    "        if top_k is not None and len(selected) >= top_k:\n",
    "            break\n",
    "            \n",
    "        # Calculate maximum correlation with already selected features\n",
    "        max_corr = 0\n",
    "        if selected:\n",
    "            for sel in selected:\n",
    "                # Handle different feature type combinations\n",
    "                if feature in numerical_features and sel in numerical_features:\n",
    "                    corr = abs(pearsonr(X_encoded[feature], X_encoded[sel])[0])\n",
    "                elif feature in categorical_features and sel in categorical_features:\n",
    "                    corr = abs(pearsonr(X_encoded[feature], X_encoded[sel])[0])\n",
    "                else:  # Mixed types - use different metric\n",
    "                    if feature in numerical_features:\n",
    "                        corr = correlation_ratio(X_encoded[sel], X_encoded[feature])\n",
    "                    else:\n",
    "                        corr = correlation_ratio(X_encoded[feature], X_encoded[sel])\n",
    "                max_corr = max(max_corr, corr)\n",
    "        \n",
    "        # Add feature if below redundancy threshold\n",
    "        if max_corr < redundancy_threshold:\n",
    "            selected.append(feature)\n",
    "    \n",
    "    return selected, normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b410c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21000\\709834610.py:53: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(X[col].fillna(X[col].median()), y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['address_line_2', 'nearest_cafe', 'nearest_mart', 'n_seven_eleven_in_1km', 'n_bank_in_1km', 'nearest_pre_school', 'nearest_secondary_school', 'nearest_primary_school', 'n_university_in_1km', 'nearest_seven_eleven', 'nearest_hotel', 'nearest_bank', 'Chroy_Changvar_Bridge_3_5km', 'Olympic_Stadium_1_2km', 'nearest_gas_station', 'Bassac_Lane_1_2km', 'Royal_Palace_1_2km', 'Koh_Pich_2_3km', 'Phsar_kandal_1_2km', 'Boeng_Keng_Kang_1_1_2km', 'f_footway', 'Sisowath_Riverside_Park_1_2km', 'Boeng_Keng_Kang_1_nearest', 'nearest_university', 'AEON_Mall_1_2_3km', 'Royal_Palace_nearest', 'Bassac_Lane_nearest', 'Olympic_Stadium_2_3km', 'AEON_Mall_1_1_2km', 'Phsar_Tmey_1_2km', 'Wat_Phnom_2_3km', 'Phnom_Penh_Airport_5_10km', 'Vattanac_Tower_2_3km', 'Russian_Market_2_3km', 'Phsar_Chas_2_3km', 'Phsar_Tmey_nearest', 'Vattanac_Tower_1_2km', 'f_tertiary', 'Phsar_Tmey_2_3km', 'Camko_City_3_5km', 'f_residential', 'Sisowath_Riverside_Park_2_3km', 'Wat_Phnom_3_5km', 'Phsar_Chas_nearest', 'Russian_Market_1_2km', 'Phsar_Chas_1_2km', 'Sisowath_Riverside_Park_3_5km', 'Chroy_Changvar_Bridge_2_3km', 'Phsar_kandal_2_3km', 'Russian_Market_3_5km', 'Koh_Pich_3_5km', 'Wat_Phnom_1_2km', 'Koh_Pich_1_2km', 'Boeng_Keng_Kang_1_2_3km', 'Bassac_Lane_2_3km', 'Royal_Palace_2_3km', 'f_service', 'Vattanac_Tower_nearest', 'Wat_Phnom_nearest', 'n_borey_in_1km_to_2km']\n",
      "\n",
      "Feature scores:\n",
      "                         pearson        mi  combined\n",
      "address_line_2          1.000000  1.000000  1.000000\n",
      "n_cafe_5km              0.908380  0.538476  0.723428\n",
      "n_secondary_school_5km  0.904251  0.521344  0.712797\n",
      "n_seven_eleven_5km      0.910294  0.510224  0.710259\n",
      "n_gas_station_5km       0.860071  0.533785  0.696928\n",
      "...                          ...       ...       ...\n",
      "f_corridor                   NaN  0.000036  0.000036\n",
      "f_bridleway                  NaN  0.000000  0.000000\n",
      "f_road                       NaN  0.000000  0.000000\n",
      "f_disused                    NaN  0.000000  0.000000\n",
      "f_unused                     NaN  0.000000  0.000000\n",
      "\n",
      "[226 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your real data\n",
    "df = pd.read_csv('../../../data/processed/land_dataset_final_v2.csv')\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(['price_per_m2', 'longitude', 'latitude', 'address_subdivision', 'h_id', 'address_locality', 'price', 'geometry'], axis=1, errors='ignore')\n",
    "y = df['price_per_m2']\n",
    "\n",
    "# Identify categorical features\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Perform feature selection\n",
    "selected_features, scores = multi_correlation_fs_regression(\n",
    "    X, \n",
    "    y,\n",
    "    metrics=['pearson', 'mi'],       # Use Pearson and MI\n",
    "    redundancy_threshold=0.8,        # Moderate redundancy control\n",
    "    top_k=60,                   \n",
    "    random_state=42,\n",
    "    categorical_features=cat_cols    # Use detected categorical columns\n",
    ")\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n",
    "print(\"\\nFeature scores:\")\n",
    "print(scores.sort_values('combined', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0d7a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved selected and encoded features to CSV.\n"
     ]
    }
   ],
   "source": [
    "# Select only the chosen features\n",
    "X_selected = X[selected_features].copy()\n",
    "\n",
    "# Identify which selected features are categorical\n",
    "cat_selected = [col for col in cat_cols if col in selected_features]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "if cat_selected:\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    ohe.fit(X_selected[cat_selected])\n",
    "    X_ohe = pd.DataFrame(\n",
    "        ohe.transform(X_selected[cat_selected]),\n",
    "        index=X_selected.index,\n",
    "        columns=ohe.get_feature_names_out(cat_selected)\n",
    "    )\n",
    "    # Drop original categorical columns and concatenate encoded columns\n",
    "    X_final = pd.concat([X_selected.drop(columns=cat_selected), X_ohe], axis=1)\n",
    "else:\n",
    "    X_final = X_selected.copy()\n",
    "\n",
    "# Add target back for saving\n",
    "final_df = pd.concat([X_final, y], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "final_df.to_csv(\"../../../data/preprocessed/feature_selection_by_multi_corr_final_data_60feature.csv\", index=False)\n",
    "print(\"Saved selected and encoded features to CSV.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_v3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
